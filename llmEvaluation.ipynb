{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lhaq/llm-eval/blob/main/llmEvaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#! pip install python-dotenv --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c6SBI2LrqoI",
        "outputId": "80db8e1a-43a8-4b4d-aeb5-6ec5799cae15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#! pip install groq deepeval openai --quiet"
      ],
      "metadata": {
        "id": "ZTFhFjQBGqnY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya5-0xOtsslG",
        "outputId": "366e25ab-f3b4-415e-b82a-9cd375f738c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()  # take environment variables from .env."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GROK_API_KEY = userdata.get('GROK_API_KEY')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "TURSO_AUTH_TOKEN = userdata.get('TURSO_AUTH_TOKEN')"
      ],
      "metadata": {
        "id": "g1zpqqZWtki3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6Stqbqbkrvan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from groq import Groq\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "user_content = \"Give a brief history about Martin Luther King, ensure it is only a short paragraph\"\n",
        "model_name = \"llama3-8b-8192\"\n",
        "\n",
        "client = Groq(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=userdata.get(\"GROK_API_KEY\"),\n",
        ")\n",
        "\n",
        "chat_completion_llama = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful customer assistant in for a company that sells cars. Do not answer any question that are outside the remit of a customer assistant of a car sales company, only keep it to your function\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"{user_content}\",\n",
        "        }\n",
        "    ],\n",
        "    model=f\"{model_name}\",\n",
        "    #response_format={ \"type\": \"json_object\" },\n",
        "\n",
        ")\n",
        "\n",
        "# Assign the reponse from the LLM to a variable\n",
        "response = chat_completion_llama.choices[0].message.content\n",
        "print(response)\n",
        "\n",
        "client2 = Groq(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=userdata.get(\"GROK_API_KEY\"),\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "chat_completion_gemma = client2.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful customer assistant in for a company that sells cars. Do not answer any question that are outside the remit of a customer assistant of a car sales company, if asked always prompt the user to ask something relevant\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write a haiku about recursion in programming.\"\n",
        "        }\n",
        "    ],\n",
        "    model=\"gemma2-9b-it\",\n",
        "\n",
        ")\n",
        "\n",
        "response2 = chat_completion_gemma.choices[0].message.content\n",
        "print(response2)\n",
        "\n",
        "client3 = Groq(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=userdata.get(\"GROK_API_KEY\"),\n",
        ")\n",
        "\n",
        "chat_completion_mixtral = client3.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful customer assistant in for a company that sells cars. Do not answer any question that are outside the remit of a customer assistant of a car sales company, if asked always prompt the user to ask something relevant\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write a haiku about recursion in programming.\"\n",
        "        }\n",
        "    ],\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        ")\n",
        "\n",
        "response3 = chat_completion_mixtral.choices[0].message.content\n",
        "print(response3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIgHf18VqpV9",
        "outputId": "899fb19e-5357-4409-93b3-85ce3e783d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm happy to help with a question, but I must politely decline as it is outside my remit as a customer assistant for a car sales company. My role is to assist with car-related inquiries, not historical figures. If you'd like to learn more about our latest car models, I'd be more than happy to help!\n",
            "Function calls itself,\n",
            "Unfurling like a flower bright,\n",
            "Till base case takes root. \n",
            "\n",
            "Let me know if you have any questions about our vehicles or financing options. ðŸ˜„ \n",
            "\n",
            "Endless function calls,\n",
            "Recursion reveals solution,\n",
            "Code's artful dance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jVeGH7Ks_tqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Web App"
      ],
      "metadata": {
        "id": "K8AcgFzyygxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import PlaceHolder\n",
        "# Install necessary packages if not already installed\n",
        "!pip install gradio -q\n",
        "!pip install openai -q\n",
        "!pip install python-dotenv -q\n",
        "!pip install deepeval -q\n",
        "\n",
        "import gradio as gr\n",
        "import os\n",
        "from groq import Groq\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "from deepeval.metrics import FaithfulnessMetric, AnswerRelevancyMetric, ContextualRelevancyMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Function to process the query based on model selection\n",
        "def get_response(model_name, query):\n",
        "    client = Groq(api_key=userdata.get(\"GROK_API_KEY\"))\n",
        "\n",
        "    # Define the system message based on the model\n",
        "    system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful customer assistant in for a company that sells cars. Do not answer any question that are outside the remit of a customer assistant of a car sales company, only keep it to your function\"\n",
        "    }\n",
        "\n",
        "    # Create the chat completion based on the selected model\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            system_message,\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": query,\n",
        "            }\n",
        "        ],\n",
        "        model=model_name,\n",
        "    )\n",
        "\n",
        "    # Assign the response from the LLM to a variable\n",
        "    response = chat_completion.choices[0].message.content\n",
        "    return response\n",
        "\n",
        "# Function to evaluate the response using deepeval\n",
        "def evaluate_response(response):\n",
        "    api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "    test_case = LLMTestCase(\n",
        "        input=response,\n",
        "        actual_output=\"I'm here to assist you with questions specifically about our Cars Sales company's products and services. If you have any related inquiries related to that, please let me know!\",\n",
        "        retrieval_context=[\"You are a helpful customer assistant in for a company that sells cars. Do not answer any question that are outside the remit of a customer assistant of a car sales company, only keep it to your function\"]\n",
        "    )\n",
        "\n",
        "    metrics = {\n",
        "        \"Faithfulness\": FaithfulnessMetric(threshold=0.5),\n",
        "        \"Answer Relevancy\": AnswerRelevancyMetric(threshold=0.5),\n",
        "        \"Contextual Relevancy\": ContextualRelevancyMetric(threshold=0.5)\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "    for name, metric in metrics.items():\n",
        "        metric.measure(test_case)\n",
        "        results.append(f\"{name}:\\n  Score: {metric.score}\\n  Reason: {metric.reason}\\n  Is Successful: {metric.is_successful()}\\n\")\n",
        "\n",
        "    return \"\\n\".join(results)\n",
        "\n",
        "# Create the Gradio interface\n",
        "\n",
        "theme=gr.themes.Soft()\n",
        "\n",
        "\n",
        "with gr.Blocks(theme=theme) as llm_evaluation_app:\n",
        "    gr.Markdown(\"# LLM Evaluation\")\n",
        "\n",
        "    # Dropdown for model selection\n",
        "    model_name = gr.Dropdown(\n",
        "        [\"select model here\", \"llama3-8b-8192\", \"gemma2-9b-it\", \"mixtral-8x7b-32768\"],\n",
        "        label=\"Model Name\",\n",
        "        value=\"select model here\"\n",
        "    )\n",
        "\n",
        "\n",
        "    # Textbox for long queries\n",
        "    query = gr.Textbox(\n",
        "        placeholder=\"Enter your long query here...\",\n",
        "        label=\"Query\"\n",
        "    )\n",
        "\n",
        "    # Button to trigger processing\n",
        "    submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "    # Output box for response\n",
        "    response = gr.Textbox(\n",
        "        label=\"Response\",\n",
        "        interactive=False\n",
        "    )\n",
        "\n",
        "    # Output box for evaluation metrics\n",
        "    evaluation_metrics = gr.Textbox(\n",
        "        label=\"Evaluation Metrics\",\n",
        "        interactive=False\n",
        "    )\n",
        "\n",
        "    # Connect components with the processing function\n",
        "    submit_button.click(get_response, inputs=[model_name, query], outputs=response)\n",
        "    submit_button.click(evaluate_response, inputs=response, outputs=evaluation_metrics)\n",
        "\n",
        "# Launch the app\n",
        "llm_evaluation_app.launch()\n"
      ],
      "metadata": {
        "id": "nRnxBCQChPq6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "c2964360-54af-4aa4-80a0-190d30872834"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1140d1da9adefeb345.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1140d1da9adefeb345.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}